/*
Navicat MySQL Data Transfer

Source Server         : localhost_3306
Source Server Version : 50553
Source Host           : localhost:3306
Source Database       : web

Target Server Type    : MYSQL
Target Server Version : 50553
File Encoding         : 65001

Date: 2019-08-16 15:53:50
*/

SET FOREIGN_KEY_CHECKS=0;

-- ----------------------------
-- Table structure for article
-- ----------------------------
DROP TABLE IF EXISTS `article`;
CREATE TABLE `article` (
  `aid` int(100) NOT NULL AUTO_INCREMENT,
  `atitle` varchar(255) DEFAULT NULL,
  `atext` longtext,
  `atime` datetime DEFAULT NULL,
  `atype` varchar(50) DEFAULT NULL,
  `alike` int(100) DEFAULT '0',
  `aimage` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`aid`)
) ENGINE=MyISAM AUTO_INCREMENT=51 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of article
-- ----------------------------
INSERT INTO `article` VALUES ('44', 'Google debuts Deep Learning Containers in beta', '<p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 1.1em; font-size: 1.6rem; line-height: 2.4rem; padding: 0px; color: rgb(74, 74, 74); font-family: Helvetica, sans-serif; white-space: normal; background-color: rgb(255, 255, 255); text-align: center;\"><span style=\"font-size: 16px;\"><img src=\"/ueditor/php/upload/image/20190710/1562773585738855.png\" title=\"1562773585738855.png\" alt=\"deep-learning-containers-768x462.png\" width=\"451\" height=\"247\"/></span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 1.1em; font-size: 1.6rem; line-height: 2.4rem; padding: 0px; color: rgb(74, 74, 74); font-family: Helvetica, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Google LLC this week announced the beta availability of a new cloud service that provides environments optimized for deploying and testing applications powered by deep learning, a subset of artificial intelligence that tries to mimic the way the human brain tackles problems.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 1.1em; font-size: 1.6rem; line-height: 2.4rem; padding: 0px; color: rgb(74, 74, 74); font-family: Helvetica, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">The service, called&nbsp;</span><a href=\"https://cloud.google.com/ai-platform/deep-learning-containers/\" style=\"box-sizing: border-box; background-color: transparent; color: rgb(51, 122, 183); font-size: 16px;\">Deep Learning Containers</a><span style=\"font-size: 16px;\">, can be run both in the cloud or on-premises. It consists of numerous performance-optimized Docker containers that come packaged with various tools necessary to run deep learning algorithms.</span></p><p style=\"box-sizing: border-box; margin-top: 1.1em; margin-bottom: 1.1em; font-size: 1.6rem; line-height: 2.4rem; padding: 0px; color: rgb(74, 74, 74); font-family: Helvetica, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Those tools include preconfigured Jupyter Notebooks, which are interactive tools used to work with and share code, equations, visualizations and text, and Google Kubernetes Engine clusters, which are used to orchestrate multiple container deployments.</span></p><p style=\"box-sizing: border-box; margin-top: 1.1em; margin-bottom: 1.1em; font-size: 1.6rem; line-height: 2.4rem; padding: 0px; color: rgb(74, 74, 74); font-family: Helvetica, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">The service also provides machine learning acceleration capabilities with Nvidia Corp.’s graphics processing units and Intel Corp.’s central processing units. Nvidia’s CUDA, cuDNN and NCCL machine learning libraries are also thrown in.</span></p><p style=\"box-sizing: border-box; margin-top: 1.1em; margin-bottom: 1.1em; font-size: 1.6rem; line-height: 2.4rem; padding: 0px; color: rgb(74, 74, 74); font-family: Helvetica, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">In a&nbsp;</span><a href=\"https://cloud.google.com/blog/products/ai-machine-learning/introducing-deep-learning-containers-consistent-and-portable-environments\" style=\"box-sizing: border-box; background-color: transparent; color: rgb(51, 122, 183); text-decoration: underline; font-size: 16px;\"><span style=\"font-size: 16px;\">blog post</span></a><span style=\"font-size: 16px;\">&nbsp;Wednesday, Google software engineer Mike Cheng explained that Deep Learning Containers are designed to provide all of the necessary dependencies needed to get applications up and running in the fastest possible time. The service also integrates with various Google Cloud services, such as BigQuery for analytics, Cloud DataProc for Apache Hadoop and Apache Spark, and Cloud Dataflow for batch processing and streaming data using Apache Beam.</span></p><p style=\"box-sizing: border-box; margin-top: 1.1em; margin-bottom: 1.1em; font-size: 1.6rem; line-height: 2.4rem; padding: 0px; color: rgb(74, 74, 74); font-family: Helvetica, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">The service supports all of the major deep learning frameworks, including PyTorch and TensorFlow, Cheng said.</span></p><p style=\"box-sizing: border-box; margin-top: 1.1em; margin-bottom: 1.1em; font-size: 1.6rem; line-height: 2.4rem; padding: 0px; color: rgb(74, 74, 74); font-family: Helvetica, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Besides running Deep Learning Containers on-premises, users have the option to host them on Google’s Compute Engine and Kubernetes Engine services, or on the Google AI Platform, which was introduced in April as a specialized cloud service for building, testing and deploying AI models.</span></p><p style=\"box-sizing: border-box; margin-top: 1.1em; margin-bottom: 1.1em; font-size: 1.6rem; line-height: 2.4rem; padding: 0px; color: rgb(74, 74, 74); font-family: Helvetica, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Some might argue that Google is late to the game with today’s offering, as its rival&nbsp;Amazon Web Services Inc.&nbsp;</span><a href=\"https://siliconangle.com/2019/03/27/aws-launches-pre-optimized-deep-learning-containers-new-infrastructure-options/\" style=\"box-sizing: border-box; background-color: transparent; color: rgb(51, 122, 183); text-decoration: underline; font-size: 16px;\"><span style=\"font-size: 16px;\">launched its own Deep Learning Containers service</span></a><span style=\"font-size: 16px;\">&nbsp;in general availability, meaning it may be more mature than Google’s. Meanwhile, Microsoft Corp. has offered its Azure Machine Learning Workspaces service&nbsp;</span><a href=\"https://siliconangle.com/2018/05/08/everything-build-microsoft-drives-ai-edge-security-forefront/\" style=\"box-sizing: border-box; background-color: transparent; color: rgb(51, 122, 183); text-decoration: underline; font-size: 16px;\"><span style=\"font-size: 16px;\">for some time</span></a><span style=\"font-size: 16px;\">.</span></p><p style=\"box-sizing: border-box; margin-top: 1.1em; margin-bottom: 1.1em; font-size: 1.6rem; line-height: 2.4rem; padding: 0px; color: rgb(74, 74, 74); font-family: Helvetica, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">But Constellation Research Inc. analyst Holger Mueller&nbsp;told SiliconANGLE that’s not the case as Amazon’s offering is more focused on running apps that have already been built, whereas Google’s offering is targeted at the developer side. As for Microsoft’s Machine Learning Workspaces, this doesn’t offer the same kind of standardization, Mueller said.</span></p><p style=\"box-sizing: border-box; margin-top: 1.1em; margin-bottom: 1.1em; font-size: 1.6rem; line-height: 2.4rem; padding: 0px; color: rgb(74, 74, 74); font-family: Helvetica, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Mueller added that with Deep Learning Containers, Google&nbsp;is making machine learning environments easier for developers&nbsp;to set up and faster to access, which is different from its rival’s offerings.</span></p><p style=\"box-sizing: border-box; margin-top: 1.1em; margin-bottom: 1.1em; font-size: 1.6rem; line-height: 2.4rem; padding: 0px; color: rgb(74, 74, 74); font-family: Helvetica, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">“This will help CxOs to add the machine learning components they need to power their next-generation applications,” Mueller said.</span></p><p style=\"box-sizing: border-box; margin-top: 1.1em; margin-bottom: 1.1em; font-size: 1.6rem; line-height: 2.4rem; padding: 0px; color: rgb(74, 74, 74); font-family: Helvetica, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">&nbsp;</span></p><h5 style=\"box-sizing: border-box; font-family: Helvetica, sans-serif; font-weight: 500; line-height: 1.1; color: rgb(51, 51, 51); margin-top: 10px; margin-bottom: 10px; font-size: 14px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Image: Google</span></h5><p><br/></p>', '2019-07-10 23:46:43', 'Computer Vision', '0', 'Uploads/2019-07-10/5d260862b0388.png');
INSERT INTO `article` VALUES ('37', 'Now your phone can become a robot that does the boring work', '<p class=\"lead\" style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 20px; font-size: 18px; line-height: 1.4; color: rgb(51, 51, 51); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; white-space: normal; background-color: rgb(255, 255, 255); border-radius: 0px !important;\"><span style=\"font-size: 20px;\"><strong>If any factory worker could program low-cost robots, then more factories could actually use robotics to increase worker productivity.</strong></span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">This is because workers would be able to shift to taking on more varied and higher-level tasks, and factories could produce a greater variety of products.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">That&#39;s the idea behind a prototype smartphone app Purdue University researchers have developed that allows a user to easily program any robot to perform a mundane activity, such as picking up parts from one area and delivering them to another.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">The setup could also take care of household chores -- no more plants dying because you forgot to water them.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">Purdue researchers present their research on the embedded app, called VRa, on June 23 at DIS 2019 in San Diego. The platform is patented through the Purdue Research Foundation Office of Technology Commercialization, with plans to make it available for commercial use.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">&quot;Smaller companies can&#39;t afford software programmers or expensive mobile robots,&quot; said Karthik Ramani, Purdue&#39;s Donald W. Feddersen Professor of Mechanical Engineering. &quot;We&#39;ve made it to where they can do the programming themselves, dramatically bringing down the costs of building and programming mobile robots,&quot; he said.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">Using augmented reality, the app allows the user to either walk out where the robot should go to perform its tasks, or draw out a workflow directly into real space. The app offers options for how those tasks can be performed, such as under a certain time limit, on repeat or after a machine has done its job.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">After programming, the user drops the phone into a dock attached to the robot. While the phone needs to be familiar with the type of robot it&#39;s &quot;becoming&quot; to perform tasks, the dock can be wirelessly connected to the robot&#39;s basic controls and motor.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">The phone is both the eyes and brain for the robot, controlling its navigation and tasks.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">&quot;As long as the phone is in the docking station, it is the robot,&quot; Ramani said. &quot;Whatever you move about and do is what the robot will do.&quot;</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">To get the robot to execute a task that involves wirelessly interacting with another object or machine, the user simply scans the QR code of that object or machine while programming, effectively creating a network of so-called &quot;Internet of Things.&quot; Once docked, the phone (as the robot) uses information from the QR code to work with the objects.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">The researchers demonstrated this with robots watering a plant, vacuuming and transporting objects. The user can also monitor the robot remotely through the app and make it start or stop a task, such as to go charge its battery or begin a 3D-printing job. The app provides an option to automatically record video when the phone is docked, so that the user can play it back and evaluate a workflow.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">Ramani&#39;s lab made it possible for the app to know how to navigate and interact with its environment according to what the user specifies through building upon so-called &quot;simultaneous localization and mapping.&quot; These types of algorithms are also used in self-driving cars and drones.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">&quot;We don&#39;t undervalue the human. Our goal is for everyone to be able to program robots, and for humans and robots to collaborate with each other,&quot; Ramani said.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">Since creating the prototype, Ramani&#39;s lab has been testing it in real factory settings to evaluate user-driven applications. Ultimately, the app is a step toward creating future &quot;smart&quot; factories, powered by artificial intelligence and augmented reality, that complement and increase worker productivity rather than replacing them, Ramani said.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">The work aligns with Purdue&#39;s Giant Leaps celebration, acknowledging the university&#39;s global advancements made in artificial intelligence as part of Purdue&#39;s 150th anniversary. This is one of the four themes of the yearlong celebration&#39;s Ideas Festival, designed to showcase Purdue as an intellectual center solving real-world issues.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">A grant from the National Science Foundation&#39;s Future of Work at the Human-Technology Frontier program is supporting the lab&#39;s continued research in enabling humans to more easily create, program and collaborate with robots.</p><hr/><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\"><strong style=\"box-sizing: border-box; border-radius: 0px !important;\">Story Source:</strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\"><a href=\"https://www.purdue.edu/newsroom/releases/2019/Q2/now-your-phone-can-become-a-robot-that-does-the-boring-work.html\" target=\"_blank\" style=\"box-sizing: border-box; background-color: transparent; color: rgb(76, 122, 159); text-decoration-line: none; border-radius: 0px !important;\">Materials</a>&nbsp;provided by&nbsp;<a href=\"http://www.purdue.edu/\" target=\"_blank\" style=\"box-sizing: border-box; background-color: transparent; color: rgb(76, 122, 159); text-decoration-line: none; border-radius: 0px !important;\"><strong style=\"box-sizing: border-box; border-radius: 0px !important;\">Purdue University</strong></a>.&nbsp;<em style=\"box-sizing: border-box; border-radius: 0px !important;\">Note: Content may be edited for style and length.</em></p><p><br/></p>', '2019-07-10 21:37:56', 'Robotics', '0', 'Uploads/2019-07-10/5d25ea3306ac4.jpg');
INSERT INTO `article` VALUES ('50', 'Banks around the world are increasingly relying on AI to drive their business', '<p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; padding: 0px; -webkit-font-smoothing: subpixel-antialiased; font-family: &quot;Frank Ruhl Libre&quot;, Georgia, serif; touch-action: manipulation; font-size: 1.5em; line-height: 1.75em; color: rgb(51, 51, 51); white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">The Banking and Finance sector (BFSI) is witnessing one of its most interesting and enriching phases. Apart from the evident shift from traditional methods of banking and payments, technology has started playing a vital role in defining this change.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; padding: 0px; -webkit-font-smoothing: subpixel-antialiased; font-family: &quot;Frank Ruhl Libre&quot;, Georgia, serif; touch-action: manipulation; font-size: 1.5em; line-height: 1.75em; color: rgb(51, 51, 51); white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Mobile apps, plastic money, e-wallets and bots have aided the phenomenal swing from offline payments to online payments over the last two decades. Now, the use of Artificial Intelligence (AI) in BFSI is expediting the evolution of this industry.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; padding: 0px; -webkit-font-smoothing: subpixel-antialiased; font-family: &quot;Frank Ruhl Libre&quot;, Georgia, serif; touch-action: manipulation; font-size: 1.5em; line-height: 1.75em; color: rgb(51, 51, 51); white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">AI enables a computer to behave and take decisions like a human being. Coined in 1956 by John McCarthy at MIT, the term AI was little known to the layman and merely a subject of interest to academicians, researchers and technologists. However, over the past few years, it is more commonly seen in our everyday lives; in our smartphones, shopping experiences, hospitals, travel, etc.</span></p><p style=\"text-align:center\"><img src=\"/ueditor/php/upload/image/20190711/1562776258643153.jpg\" title=\"1562776258643153.jpg\" alt=\"27208905-istockphoto-artificial-intelligence-800x450.jpg\" width=\"505\" height=\"261\"/></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; padding: 0px; -webkit-font-smoothing: subpixel-antialiased; font-family: &quot;Frank Ruhl Libre&quot;, Georgia, serif; touch-action: manipulation; font-size: 1.5em; line-height: 1.75em; color: rgb(51, 51, 51); white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">The BFSI sector uses AI because of its inherent advantages – efficiency and faster processing, accuracy, daily application and data availability. Here are some artificial intelligence technologies and what they offer for Banking and Finance.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; padding: 0px; -webkit-font-smoothing: subpixel-antialiased; font-family: &quot;Frank Ruhl Libre&quot;, Georgia, serif; touch-action: manipulation; font-size: 1.5em; line-height: 1.75em; color: rgb(51, 51, 51); white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Machine Learning, Deep Learning, NLP Platforms, Predictive APIs and Image and Speech Recognition are some core AI technologies used in BFSI today. Machine Learning recognises data patterns and highlights deviations in data observed. Data is analysed and then compared with existing data to look for patterns. This can help in fraud detection, prediction of spending patterns and subsequently, the development of new products.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; padding: 0px; -webkit-font-smoothing: subpixel-antialiased; font-family: &quot;Frank Ruhl Libre&quot;, Georgia, serif; touch-action: manipulation; font-size: 1.5em; line-height: 1.75em; color: rgb(51, 51, 51); white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Key Stroke Dynamics can be used for analysing transactions made by customers. They capture strokes when the key is pressed (dwell time) and released on a keyboard, along with vibration information.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; padding: 0px; -webkit-font-smoothing: subpixel-antialiased; font-family: &quot;Frank Ruhl Libre&quot;, Georgia, serif; touch-action: manipulation; font-size: 1.5em; line-height: 1.75em; color: rgb(51, 51, 51); white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">As second factor authentication is mandatory for electronic payments in India, this can help detect fraud, especially if the user’s credentials are compromised. Deep Learning is a new area in Machine Learning research and consists of multiple linear and non-linear transformations. It is based on learning and improving representations of data. A common application of this can be found in the crypto-currency, Bitcoin.</span></p><p style=\"text-align:center\"><img src=\"/ueditor/php/upload/image/20190711/1562776327137357.jpg\" title=\"1562776327137357.jpg\" alt=\"RTX2CI5N.jpg\" width=\"521\" height=\"290\"/></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; padding: 0px; -webkit-font-smoothing: subpixel-antialiased; font-family: &quot;Frank Ruhl Libre&quot;, Georgia, serif; touch-action: manipulation; font-size: 1.5em; line-height: 1.75em; color: rgb(51, 51, 51); white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Adaptive Learning is another form of AI currently used by Indian banks for fraud detection and mitigation. A model is created using existing rules or data in the bank’s system. Incremental learning algorithms are then used to update the models based on changes observed in the data patterns.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; padding: 0px; -webkit-font-smoothing: subpixel-antialiased; font-family: &quot;Frank Ruhl Libre&quot;, Georgia, serif; touch-action: manipulation; font-size: 1.5em; line-height: 1.75em; color: rgb(51, 51, 51); white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Natural Language Processing (NLP) is the ability of a computer to understand human speech as it is spoken. In India, ICICI has started using NLP for sentiment mapping. It is predicted that the NLP market will reach $13.4 billion by 2020 at 18.4 percent CAGR (Compound Annual Growth Rate).</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; padding: 0px; -webkit-font-smoothing: subpixel-antialiased; font-family: &quot;Frank Ruhl Libre&quot;, Georgia, serif; touch-action: manipulation; font-size: 1.5em; line-height: 1.75em; color: rgb(51, 51, 51); white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">NLP and AI bots may also be used for Whatsapp-based banking, similar to how WeChat is used for banking in China. Voice Recognition used for secure banking transactions can take payment security to the next level. The Santander Group, based in Spain, has already announced the usage of this feature in their app. Similarly, AI can be used to answer customer questions as well. Swedbank uses Nina (a web assistant) to artificially interact with customers and build loyalty.</span></p><p><br/></p>', '2019-07-11 00:33:19', 'Natural Language Processing', '0', 'Uploads/2019-07-11/5d26134e0096f.jpg');
INSERT INTO `article` VALUES ('49', 'Evolution of next gen chatbots: How natural language processing is changing the game', '<p><span style=\"font-family: &quot;Titillium Web&quot;, sans-serif; background-color: rgb(255, 255, 255);\">With Artificial Intelligence (AI) revolutionizing all walks of business and social life, enterprises are plugging in conversational AI to enhance cross channel customer experience.&nbsp;</span><strong style=\"box-sizing: border-box; font-family: &quot;Titillium Web&quot;, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><a target=\"_blank\" href=\"https://www.gartner.com/smarterwithgartner/gartner-top-strategic-predictions-for-2018-and-beyond/\" style=\"box-sizing: border-box; color: rgb(0, 123, 255); text-decoration-line: none; background-color: transparent;\">Gartner</a></strong><span style=\"font-family: &quot;Titillium Web&quot;, sans-serif; background-color: rgb(255, 255, 255);\">&nbsp;predicts that more than 50 percent of enterprises will opt for chatbots as the preferred channel over traditional mobile apps.</span></p><p style=\"text-align: center;\"><span style=\"font-family: &quot;Titillium Web&quot;, sans-serif; background-color: rgb(255, 255, 255);\"><img src=\"/ueditor/php/upload/image/20190711/1562776026963094.jpg\" title=\"1562776026963094.jpg\" alt=\"Chatbots-Gupshup-380-285.jpg\"/></span></p><p><span style=\"font-family: &quot;Titillium Web&quot;, sans-serif; background-color: rgb(255, 255, 255);\"></span></p><p class=\"wp-caption-text\" style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; width: 725.2px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Representational image.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">The technology at the core of the rise of the chatbot is natural language processing (NLP). NLP infused chatbots, designed to mimic human conversations, have gone through the peak of Gartner hype cycle expectations, the trough of disillusionment and are heading towards enlightenment and productivity.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">With more people using chat to communicate, chatbots have gained a lot of importance and are fast becoming a trend for digitally empowered consumers. NLP provides a way for chatbots to understand and interpret the user in their own language to offer a richer conversational experience.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\"><strong style=\"box-sizing: border-box;\">Basic building blocks of chatbots</strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">Chatbots have a front-end conversational interface that connects to a variety of channels such as Facebook Messenger, Slack, Skype etc., with NLP to parse user messages and conversational logic.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">Firstly, chatbots need to understand user input. It can achieve this through a basic technique of pattern matching or advanced intent classification technique that uses machine learning (ML).</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">A pattern matching technique needs a list of possible input patterns. They are easy to read and maintain. The problem is that they are built manually and do not scale in real use cases.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">An intent classification approach relies on machine learning techniques and you need a set of user utterances to train a classifier.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">Once it understands what the user says, it can generate a response, based on the current input and the context of the conversation. The simplest way is to have scripted static response for each user input. Another approach, would be to use a knowledge base to generate a dynamic response based on the context.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\"><strong style=\"box-sizing: border-box;\">Chatbots come with different level of intelligence</strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">The way we use chatbots and the method they use to respond to the user is changing. Initially, they would provide predetermined responses that are handcrafted as explained in the earlier section.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">A more advanced approach is to use the deep learning technique to train a generative model and get a list of potential responses and then score them to choose the best suited response. Deep learning algorithms mean that chatbots are able to learn from every interaction, and incorporate that feedback so that their performance is continually improved. This cycle of continual improvement means that they are refining themselves over time.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">Chatbots based on generative models are smarter but you need millions of examples of meaningful conversational data to attain a decent quality. This is an extremely promising area of current research which gives hope that the bot will get better at imitating humans.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\"><strong style=\"box-sizing: border-box;\">Understanding the vendor landscape before you implement</strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">In practice, chatbots are either goal oriented or conversational in nature. A goal-oriented or transactional chatbot helps a user perform specific tasks such as booking a ticket or ordering food.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">A conversational chatbot, on the other hand, does not necessarily have a well-defined intention. It focuses on having an open domain conversation with the user and does not have to remember all the context of conversation.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">To create a successful chatbot, the market is flooded with an array of platforms and tools, having different complexity levels, conversational intelligence and integration capabilities. Leading technology giants like IBM, Google, Microsoft, Amazon, Facebook are investing in this space to provide NLP, bot &amp; AI frameworks, deployment platforms and as-a-service platforms.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">The most common ones include Dialogflow (Google, ex API.ai), Amazon Alexa, Luis.ai (Microsoft), Wit.ai (Facebook), and IBM Watson.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">There are a lot of aspects to consider when implementing a chatbot. It is important to find the platform that fits your particular need. Open source NLP libraries such as spaCy and AllenNLP are available for companies who prefer a customized chatbot solution.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">In conclusion, chatbots are poised to revolutionize user interface design. As chatbots and intelligent automation are on the rise, enterprises should look at NLP infused chatbots to drive cost saving, operational efficiencies, and enhanced customer experiences throughout their businesses.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\"><em style=\"box-sizing: border-box;\">The author is the principal architect at Persistent Systems.</em></p><p class=\"whats-app-desc\" style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\"><em style=\"box-sizing: border-box;\">As we follow the planned launch of India&#39;s second mission to the Moon, Chandrayaan-2 on 15 July, you can find our entire collection of stories, in-depth analysis, live updates, videos &amp; more on our dedicated&nbsp;<a href=\"https://www.firstpost.com/tech/chandrayaan-2\" class=\"external-link\" title=\"Follow link\" style=\"box-sizing: border-box; color: rgb(0, 123, 255); text-decoration-line: none; background-color: transparent;\">#Chandrayaan2TheMoon</a>&nbsp;domain.</em></p><p><span style=\"font-family: &quot;Titillium Web&quot;, sans-serif; background-color: rgb(255, 255, 255);\"><br/></span><br/></p>', '2019-07-11 00:28:00', 'Natural Language Processing', '1', 'Uploads/2019-07-11/5d26120f3b0c9.jpg');
INSERT INTO `article` VALUES ('48', 'Facebook reportedly acquiring London-based AI startup Bloomsbury', '<p><span style=\"font-family: &quot;Titillium Web&quot;, sans-serif; background-color: rgb(255, 255, 255);\">Facebook is reportedly acquiring London-based startup Bloomsbury AI that has developed natural language processing (NLP) technology to help machines answer questions.</span></p><p><span style=\"font-family: &quot;Titillium Web&quot;, sans-serif; background-color: rgb(255, 255, 255);\"><br/></span></p><p style=\"text-align: center;\"><span style=\"font-family: &quot;Titillium Web&quot;, sans-serif; background-color: rgb(255, 255, 255);\"><img src=\"/ueditor/php/upload/image/20190711/1562775827355442.jpg\" title=\"1562775827355442.jpg\" alt=\"facebook-12801.jpg\" width=\"631\" height=\"348\"/></span></p><p><span style=\"font-family: &quot;Titillium Web&quot;, sans-serif; background-color: rgb(255, 255, 255);\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span><span style=\"font-family: &quot;Titillium Web&quot;, sans-serif;\">A 3D-printed Facebook logo. Reuters</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\"><br/></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">According to a report in&nbsp;<em style=\"box-sizing: border-box;\">TechCrunch</em>&nbsp;on Monday, &quot;multiple sources say Facebook is paying between $23 million and $30 million to acquire Bloomsbury AI&quot;.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">The social media giant aims to deploy the Bloomsbury AI technology to work on fighting fake news, the report added.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">Facebook was yet to comment to the report.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">Facebook recently announced that it was using AI to remove posts from its platform that involve hate speech, nudity, graphic violence, terrorist content, spam, fake accounts and suicide.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">&quot;We view AI as a foundational technology, and we&#39;ve made deep investments in advancing the state of the art through scientist-directed research,&quot; Facebook said in a statement.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">At its F8 developer conference in May, Facebook&#39;s AI research and engineering teams shared a recent breakthrough: the teams successfully trained an image recognition system on a data set of 3.5 billion publicly available photos, using the hashtags on those photos in place of human annotations.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\">After making deep investments in AI technology, Facebook has also announced the next version of its open-source AI framework for developers.</p><p class=\"whats-app-desc\" style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; color: rgb(0, 0, 0);\"><em style=\"box-sizing: border-box;\">As we follow the planned launch of India&#39;s second mission to the Moon, Chandrayaan-2 on 15 July, you can find our entire collection of stories, in-depth analysis, live updates, videos &amp; more on our dedicated&nbsp;<a href=\"https://www.firstpost.com/tech/chandrayaan-2\" class=\"external-link\" title=\"Follow link\" style=\"box-sizing: border-box; color: rgb(0, 123, 255); text-decoration-line: none; background-color: transparent;\">#Chandrayaan2TheMoon</a>&nbsp;domain.</em></p><p><span style=\"font-family: &quot;Titillium Web&quot;, sans-serif; background-color: rgb(255, 255, 255);\"><br/></span><br/></p>', '2019-07-11 00:24:56', 'Natural Language Processing', '0', 'Uploads/2019-07-11/5d26115754523.jpg');
INSERT INTO `article` VALUES ('46', 'A Brief History of Computer Vision (and Convolutional Neural Networks)', '<p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255); text-align: center;\"><img src=\"/ueditor/php/upload/image/20190710/1562774315624950.jpeg\" title=\"1562774315624950.jpeg\" alt=\"1_YFO9qMzYdUtuzy3daSnpQw.jpeg\" width=\"744\" height=\"292\"/></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Although Computer Vision (CV) has only exploded recently (the breakthrough moment happened in 2012 when&nbsp;</span><a href=\"https://en.wikipedia.org/wiki/AlexNet\" class=\"cz bu jv jw jx jy\" style=\"box-sizing: inherit; text-decoration: underline; -webkit-tap-highlight-color: transparent; background-repeat: repeat-x; background-image: url(&quot;data:image/svg+xml;utf8,&lt;svg preserveAspectRatio=\\&quot;none\\&quot; viewBox=\\&quot;0 0 1 1\\&quot; xmlns=\\&quot;http://www.w3.org/2000/svg\\&quot;&gt;&lt;line x1=\\&quot;0\\&quot; y1=\\&quot;0\\&quot; x2=\\&quot;1\\&quot; y2=\\&quot;1\\&quot; stroke=\\&quot;rgba(0, 0, 0, 0.84)\\&quot; /&gt;&lt;/svg&gt;&quot;); background-size: 1px 1px; background-position: 0px calc(1em + 1px); font-size: 16px;\"><span style=\"font-size: 16px;\">AlexNet won ImageNet</span></a><span style=\"font-size: 16px;\">), it certainly isn’t a new scientific field.</span></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Computer scientists around the world have been trying to find ways to make machines extract meaning from visual data for about 60 years now, and the history of Computer Vision, which most people don’t know much about, is deeply fascinating.</span></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">In this article, I’ll try to shed some light on how modern CV systems, powered primarily by convolutional neural networks, came to be.</span></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">I’ll start with a work that came out in the late 1950s and has nothing to do with software engineering or&nbsp;</span><a href=\"https://www.kualitatem.com/\" class=\"cz bu jv jw jx jy\" style=\"box-sizing: inherit; text-decoration: underline; -webkit-tap-highlight-color: transparent; background-repeat: repeat-x; background-image: url(&quot;data:image/svg+xml;utf8,&lt;svg preserveAspectRatio=\\&quot;none\\&quot; viewBox=\\&quot;0 0 1 1\\&quot; xmlns=\\&quot;http://www.w3.org/2000/svg\\&quot;&gt;&lt;line x1=\\&quot;0\\&quot; y1=\\&quot;0\\&quot; x2=\\&quot;1\\&quot; y2=\\&quot;1\\&quot; stroke=\\&quot;rgba(0, 0, 0, 0.84)\\&quot; /&gt;&lt;/svg&gt;&quot;); background-size: 1px 1px; background-position: 0px calc(1em + 1px); font-size: 16px;\"><span style=\"font-size: 16px;\">software testing</span></a><span style=\"font-size: 16px;\">.</span></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">One of the most influential papers in Computer Vision was published by two neurophysiologists — David Hubel and Torsten Wiesel — in 1959. Their publication, entitled “</span><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/\" class=\"cz bu jv jw jx jy\" style=\"box-sizing: inherit; text-decoration: underline; -webkit-tap-highlight-color: transparent; background-repeat: repeat-x; background-image: url(&quot;data:image/svg+xml;utf8,&lt;svg preserveAspectRatio=\\&quot;none\\&quot; viewBox=\\&quot;0 0 1 1\\&quot; xmlns=\\&quot;http://www.w3.org/2000/svg\\&quot;&gt;&lt;line x1=\\&quot;0\\&quot; y1=\\&quot;0\\&quot; x2=\\&quot;1\\&quot; y2=\\&quot;1\\&quot; stroke=\\&quot;rgba(0, 0, 0, 0.84)\\&quot; /&gt;&lt;/svg&gt;&quot;); background-size: 1px 1px; background-position: 0px calc(1em + 1px); font-size: 16px;\"><span style=\"font-size: 16px;\"><em class=\"jz\" style=\"box-sizing: inherit;\">Receptive fields of single neurons in the cat’s striate cortex</em></span></a><span style=\"font-size: 16px;\"><em class=\"jz\" style=\"box-sizing: inherit;\">”,&nbsp;</em>described core response properties of visual cortical neurons as well how a cat’s visual experience shapes its cortical architecture.</span></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">The duo ran some pretty elaborate experiments. They placed electrodes into the primary visual cortex area of an anesthetized cat’s brain and observed, or at least tried to, the neuronal activity in that region while showing the animal various images. Their first efforts were fruitless; they couldn’t get the nerve cells to respond to anything.</span></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">However, a few months into the research, they noticed, rather accidentally, that one neuron fired as they were slipping a new slide into the projector. This was one lucky accident! After some initial confusion, Hubel and Wiesel realized that what got the neuron excited was the movement of the line created by the shadow of the sharp edge of the glass slide.</span></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255); text-align: center;\"><img src=\"/ueditor/php/upload/image/20190710/1562774390982544.jpg\" title=\"1562774390982544.jpg\" alt=\"0_hJWJ-TCD7af6jlPf.jpg\" width=\"589\" height=\"327\"/></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255);\"><a href=\"https://goodpsychology.wordpress.com/2013/03/13/235/\" class=\"cz bu jv jw jx jy\" style=\"box-sizing: inherit; text-decoration: underline; -webkit-tap-highlight-color: transparent; background-repeat: repeat-x; background-image: url(&quot;data:image/svg+xml;utf8,&lt;svg preserveAspectRatio=\\&quot;none\\&quot; viewBox=\\&quot;0 0 1 1\\&quot; xmlns=\\&quot;http://www.w3.org/2000/svg\\&quot;&gt;&lt;line x1=\\&quot;0\\&quot; y1=\\&quot;0\\&quot; x2=\\&quot;1\\&quot; y2=\\&quot;1\\&quot; stroke=\\&quot;rgba(0, 0, 0, 0.84)\\&quot; /&gt;&lt;/svg&gt;&quot;); background-size: 1px 1px; background-position: 0px calc(1em + 1px); font-size: 16px;\"><span style=\"font-size: 16px;\"><em class=\"jz\" style=\"box-sizing: inherit;\">Image Source</em></span></a></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">The researchers established, through their experimentation, that there are simple and complex neurons in the primary visual cortex and that visual processing always starts with simple structures such as oriented edges.</span></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Sounds familiar? Well, yeah, this is essentially the core principle behind deep learning.</span></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">The next highlight in the history of CV was the invention of the first digital image scanner.</span></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">In 1959, Russell Kirsch and his colleagues developed an apparatus that allowed transforming images into grids of numbers — the binary language machines could understand. And it’s because of their work that we now can process digital images in various ways.</span></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">One of the first digitally scanned photos was the image of&nbsp;</span><a href=\"https://www.nist.gov/node/774341\" class=\"cz bu jv jw jx jy\" style=\"box-sizing: inherit; text-decoration: underline; -webkit-tap-highlight-color: transparent; background-repeat: repeat-x; background-image: url(&quot;data:image/svg+xml;utf8,&lt;svg preserveAspectRatio=\\&quot;none\\&quot; viewBox=\\&quot;0 0 1 1\\&quot; xmlns=\\&quot;http://www.w3.org/2000/svg\\&quot;&gt;&lt;line x1=\\&quot;0\\&quot; y1=\\&quot;0\\&quot; x2=\\&quot;1\\&quot; y2=\\&quot;1\\&quot; stroke=\\&quot;rgba(0, 0, 0, 0.84)\\&quot; /&gt;&lt;/svg&gt;&quot;); background-size: 1px 1px; background-position: 0px calc(1em + 1px); font-size: 16px;\"><span style=\"font-size: 16px;\">Russell’s infant son</span></a><span style=\"font-size: 16px;\">. It was just a grainy 5cm by 5cm photo captured as 30,976 pixels (176x176 array), but it has become so incredibly famous that the original image is now stored in the Portland Art Museum.</span></p><p></p><p><span style=\"font-size: 16px;\"><em class=\"jz\" style=\"color: rgba(0, 0, 0, 0.84); font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; letter-spacing: -0.004em; box-sizing: inherit;\">Conclusion</em></span></p><p class=\"jh ji ed aq jj b jk jl jm jn jo jp jq jr js jt ju\" style=\"box-sizing: inherit; margin-top: 2em; margin-bottom: -0.46em; color: rgba(0, 0, 0, 0.84); line-height: 1.58; letter-spacing: -0.004em; font-family: medium-content-serif-font, Georgia, Cambria, &quot;Times New Roman&quot;, Times, serif; font-size: 21px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Despite the recent progress, which has been impressive, we’re still not even close to&nbsp;</span><a href=\"https://perfectial.com/blog/computer-vision-applications/\" class=\"cz bu jv jw jx jy\" style=\"box-sizing: inherit; text-decoration: underline; -webkit-tap-highlight-color: transparent; background-repeat: repeat-x; background-image: url(&quot;data:image/svg+xml;utf8,&lt;svg preserveAspectRatio=\\&quot;none\\&quot; viewBox=\\&quot;0 0 1 1\\&quot; xmlns=\\&quot;http://www.w3.org/2000/svg\\&quot;&gt;&lt;line x1=\\&quot;0\\&quot; y1=\\&quot;0\\&quot; x2=\\&quot;1\\&quot; y2=\\&quot;1\\&quot; stroke=\\&quot;rgba(0, 0, 0, 0.84)\\&quot; /&gt;&lt;/svg&gt;&quot;); background-size: 1px 1px; background-position: 0px calc(1em + 1px); font-size: 16px;\"><span style=\"font-size: 16px;\">solving computer vision</span></a><span style=\"font-size: 16px;\">. However, there are already multiple healthcare institutions and enterprises that have found ways to apply CV systems, powered by CNNs, to real-world problems. And this trend is not likely to stop anytime soon.</span></p><p><br/></p>', '2019-07-11 00:02:08', 'Computer Vision', '2', 'Uploads/2019-07-11/5d260bff7bd30.jpeg');
INSERT INTO `article` VALUES ('47', 'AI chatbots will transform the way people travel, with voice dominating searches', '<p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">As digital technologies transform almost every aspect of our daily lives, as customers, our expectations regarding speed, convenience and customisation are constantly rising. We have grown used to entertaining, communicating and purchasing on mobile devices. To remain competitive, in the travel industry too, digital technologies such as artificial intelligence (AI) and machine learning (ML) are being increasingly explored.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">When developed well, AI-powered solutions in the travel services industry can provide a high level of customer value as they often leverage Natural Language Processing (NLP) engines to mimic human interactions with minimal error, high security and problem-solving capabilities.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">It is not an exaggeration to say that AI has turned traditional business strategy on its head. What I mean by this is: most often, solutions were conceptualised at a demographic level, i.e. customised for a group of people in a similar age/interest level. But now, AI allows organisations to customise solutions for an individual, recognising the highly unique preferences each of us has.</span></p><p style=\"text-align: center;\"><img src=\"/ueditor/php/upload/image/20190711/1562775541414094.jpg\" title=\"1562775541414094.jpg\" alt=\"1556799374_jeshoots-com-722888-unsplash.jpg\" width=\"543\" height=\"297\"/></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">This transformation is steadily extending to government-to-citizen services as well, including the traditional visa processes space, defined by its very nature as having multiple layers of checks. For the visa processing industry and indeed the entire gamut of related travel services, the potential of AI use is enormous. Websites and mobile apps powered by AI can assist you at every step of filling up a visa application form or produce customised stay or flight options based on your past preferences, and help you navigate through foreign cities.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Today’s sophisticated chatbots have the ability to think two steps ahead – if you are trying to track your passport or visa, for instance, a chatbot can not only give you a status update but can also potentially provide information you did not ask for – such as, what are your pick -up or courier delivery options once the document is processed. Similarly, if you’re filling up a visa application form or a hotel booking form, and pause too long over a certain entry, a chatbot can automatically pop up and offer to assist you through the rest of the process.</span></p><h2 style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 0.5rem; font-family: Tahoma, sans-serif; font-weight: 500; line-height: 1.2; color: rgb(33, 37, 41); font-size: 2rem; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 20px;\"><strong style=\"box-sizing: border-box;\">The future is voice</strong></span></h2><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Voice-activated searches, powered by AI and supported by natural language processing (NLP) engines, are becoming ubiquitous – most commonly found on many smartphones today. Voice assistants like Siri and Alexa have been trail-blazers, helping you to book an&nbsp;<em style=\"box-sizing: border-box;\">Uber</em>&nbsp;or a flight, tell the time, or find a recipe, all with a simple voice command.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Simply put, NLP engines can recognise contemporary language usages, accents, colloquialisms, translations and tonality so that the responses provided by a voice assistant are highly nuanced and as close to human interaction as possible. A simple example is if you ask “How are you today?” or “What’s up?” or “What’s going on?”, a voice assistant’s response would be the same, along the lines of “I’m fine”, though the phrasing is different.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">No wonder then that in 2016, Google reported that&nbsp;</span><a target=\"_blank\" href=\"https://searchengineland.com/google-reveals-20-percent-queries-voice-queries-249917\" style=\"box-sizing: border-box; color: rgb(0, 123, 255); text-decoration: underline; background-color: transparent; font-size: 16px;\"><span style=\"font-size: 16px;\"><strong style=\"box-sizing: border-box;\">20 percent of its mobile app queries</strong></span></a><span style=\"font-size: 16px;\">&nbsp;in the US were voice searches. By 2020, estimates show that 50 percent of all searches across the internet will be voice-based – a sign of how fast the voice assistant market is growing. Travel-related companies are already making technology investments in such apps and other AI-powered features as end users come to depend more on such services.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">AI is already driving revenue generation by helping companies make changes in their web interfaces such that the most useful services appear more prominent to the browser, keeping in mind his/her preferences based on past online activity.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">For the modern traveller perennially short on time, these solutions allow you to experience the joy of travel from ‘inspiration’ to ‘execution’, using technology to keep convenience at your fingertips, literally.</span></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\"><em style=\"box-sizing: border-box;\">The author is the head - Digital and eCommerce at VFS Global</em></span></p><p class=\"whats-app-desc\" style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 15px; font-family: &quot;Titillium Web&quot;, sans-serif; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\"><em style=\"box-sizing: border-box;\">As we follow the planned launch of India&#39;s second mission to the Moon, Chandrayaan-2 on 15 July, you can find our entire collection of stories, in-depth analysis, live updates, videos &amp; more on our dedicated&nbsp;</em><em style=\"box-sizing: border-box;\"><a href=\"https://www.firstpost.com/tech/chandrayaan-2\" class=\"external-link\" title=\"Follow link\" style=\"box-sizing: border-box; color: rgb(0, 123, 255); text-decoration-line: none; background-color: transparent;\">#Chandrayaan2TheMoon</a>&nbsp;domain.</em></span></p><p><br/></p>', '2019-07-11 00:20:38', 'Natural Language Processing', '0', 'Uploads/2019-07-11/5d2610556f6ae.jpg');
INSERT INTO `article` VALUES ('45', 'How to hide from the AI surveillance state with a color printout', '<p class=\"jsx-1279085685\" style=\"box-sizing: inherit; margin-top: 0px; margin-bottom: 14px; color: rgb(48, 48, 48); font-family: NeueHaas, sans-serif; font-size: 18px; white-space: normal; background-color: rgb(255, 255, 255); text-align: center;\"><span style=\"font-size: 16px;\"><img src=\"/ueditor/php/upload/image/20190710/1562773999845144.jpg\" title=\"1562773999845144.jpg\" alt=\"acr13282673947520124191772.jpg\" width=\"587\" height=\"308\"/></span></p><p class=\"jsx-1279085685\" style=\"box-sizing: inherit; margin-top: 0px; margin-bottom: 14px; color: rgb(48, 48, 48); font-family: NeueHaas, sans-serif; font-size: 18px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">AI-powered video technology is becoming ubiquitous, tracking our faces and bodies through stores, offices, and public spaces. In some countries the technology constitutes a powerful new layer of policing and government surveillance.</span></p><p class=\"jsx-1279085685\" style=\"box-sizing: inherit; margin-top: 0px; margin-bottom: 14px; color: rgb(48, 48, 48); font-family: NeueHaas, sans-serif; font-size: 18px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Fortunately, as some researchers from the Belgian university KU Leuven&nbsp;&nbsp;<span class=\"jsx-2473920458 storyLink\" style=\"font-size: 16px; box-sizing: inherit;\"><a class=\"\" href=\"https://arxiv.org/abs/1904.08653\" style=\"box-sizing: inherit;\">have just shown</a></span>, you can often hide from an AI video system with the aid of a simple color printout.</span></p><p class=\"jsx-1279085685\" style=\"box-sizing: inherit; margin-top: 0px; margin-bottom: 14px; color: rgb(48, 48, 48); font-family: NeueHaas, sans-serif; font-size: 18px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\"><strong style=\"box-sizing: inherit;\">Who said that?</strong>&nbsp;The researchers showed that the image they designed can hide a whole person from an AI-powered computer-vision system. They demonstrated it on a popular open-source object recognition system called&nbsp;<span class=\"jsx-2473920458 storyLink\" style=\"font-size: 16px; box-sizing: inherit;\"><a class=\"\" href=\"https://pjreddie.com/darknet/yolov2/\" style=\"box-sizing: inherit;\">YoLo(v2)</a></span>.</span></p><p class=\"jsx-1279085685\" style=\"box-sizing: inherit; margin-top: 0px; margin-bottom: 14px; color: rgb(48, 48, 48); font-family: NeueHaas, sans-serif; font-size: 18px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\"><strong style=\"box-sizing: inherit;\">Hide and seek:</strong>&nbsp;The trick could conceivably let crooks hide from security cameras, or offer dissidents a way to dodge government scrutiny. “What our work proves is that it is possible to bypass camera surveillance systems using adversarial patches,” says&nbsp;<span class=\"jsx-2473920458 storyLink\" style=\"font-size: 16px; box-sizing: inherit;\"><a class=\"\" href=\"https://www.kuleuven.be/wieiswie/en/person/00093819\" style=\"box-sizing: inherit;\">Wiebe Van Ranst</a></span>, one of the authors.</span></p><p class=\"jsx-1279085685\" style=\"box-sizing: inherit; margin-top: 0px; margin-bottom: 14px; color: rgb(48, 48, 48); font-family: NeueHaas, sans-serif; font-size: 18px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\"><strong style=\"box-sizing: inherit;\">Get lost:</strong>&nbsp;Van Ranst says it shouldn’t be too hard to adapt the approach to off-the-shelf video surveillance systems. “At the moment we also need to know which detector is in use. What we’d like to do in the future is generate a patch that works on multiple detectors at the same time,” he told MIT Technology Review. “If this works, chances are high that the patch will also work on the detector that is in use in the surveillance system.”</span></p><p class=\"jsx-1279085685\" style=\"box-sizing: inherit; margin-top: 0px; margin-bottom: 14px; color: rgb(48, 48, 48); font-family: NeueHaas, sans-serif; font-size: 18px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\"><strong style=\"box-sizing: inherit;\">Fool’s errand:</strong>&nbsp;The deception demonstrated by the Belgian team exploits what’s known as adversarial machine learning. Most computer vision relies on training a (convolutional) neural network to recognize different things by feeding it examples and tweaking its parameters until it classifies objects correctly. By feeding examples into a trained deep neural net and monitoring the output, it is possible to infer what types of images confuse or fool the system.</span></p><p class=\"jsx-1279085685\" style=\"box-sizing: inherit; margin-top: 0px; margin-bottom: 14px; color: rgb(48, 48, 48); font-family: NeueHaas, sans-serif; font-size: 18px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\"><strong style=\"box-sizing: inherit;\">Eyes everywhere:</strong>&nbsp;The work is significant because AI is increasingly found in everyday surveillance cameras and software. It’s even being used to obviate the need for a checkout line in some experimental stores, including&nbsp;<span class=\"jsx-2473920458 storyLink\" style=\"font-size: 16px; box-sizing: inherit;\"><a class=\"\" href=\"https://www.technologyreview.com/s/610006/amazons-checkout-free-grocery-store-is-opening-to-the-public/\" style=\"box-sizing: inherit;\">ones operated by Amazon</a></span>. And in China the technology is emerging as a powerful new means of catching criminals as well as, more troublingly,&nbsp;<span class=\"jsx-2473920458 storyLink\" style=\"font-size: 16px; box-sizing: inherit;\"><a class=\"\" href=\"https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-intelligence-racial-profiling.html\" style=\"box-sizing: inherit;\">tracking certain ethnic groups</a></span>.</span></p><p><br/></p>', '2019-07-10 23:53:38', 'Computer Vision', '0', 'Uploads/2019-07-10/5d260a0171148.jpg');
INSERT INTO `article` VALUES ('38', 'Algorithm tells robots where nearby humans are headed', '<p class=\"lead\" style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 20px; font-size: 18px; line-height: 1.4; color: rgb(51, 51, 51); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; white-space: normal; background-color: rgb(255, 255, 255); border-radius: 0px !important;\"><strong>In 2018, researchers at MIT and the auto manufacturer BMW were testing ways in which humans and robots might work in close proximity to assemble car parts. In a replica of a factory floor setting, the team rigged up a robot on rails, designed to deliver parts between work stations. Meanwhile, human workers crossed its path every so often to work at nearby stations.</strong></p><p class=\"lead\" style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 20px; font-size: 18px; line-height: 1.4; color: rgb(51, 51, 51); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; white-space: normal; background-color: rgb(255, 255, 255); border-radius: 0px !important; text-align: center;\"><strong><img src=\"/ueditor/php/upload/image/20190710/1562765148583726.jpg\" title=\"1562765148583726.jpg\" alt=\"190612141314_1_540x360.jpg\"/></strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">The robot was programmed to stop momentarily if a person passed by. But the researchers noticed that the robot would often freeze in place, overly cautious, long before a person had crossed its path. If this took place in a real manufacturing setting, such unnecessary pauses could accumulate into significant inefficiencies.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">The team traced the problem to a limitation in the robot&#39;s trajectory alignment algorithms used by the robot&#39;s motion predicting software. While they could reasonably predict where a person was headed, due to the poor time alignment the algorithms couldn&#39;t anticipate how long that person spent at any point along their predicted path -- and in this case, how long it would take for a person to stop, then double back and cross the robot&#39;s path again.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">Now, members of that same MIT team have come up with a solution: an algorithm that accurately aligns partial trajectories in real-time, allowing motion predictors to accurately anticipate the timing of a person&#39;s motion. When they applied the new algorithm to the BMW factory floor experiments, they found that, instead of freezing in place, the robot simply rolled on and was safely out of the way by the time the person walked by again.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">&quot;This algorithm builds in components that help a robot understand and monitor stops and overlaps in movement, which are a core part of human motion,&quot; says Julie Shah, associate professor of aeronautics and astronautics at MIT. &quot;This technique is one of the many way we&#39;re working on robots better understanding people.&quot;</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">Shah and her colleagues, including project lead and graduate student Przemyslaw &quot;Pem&quot; Lasota, will present their results this month at the Robotics: Science and Systems conference in Germany.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\"><strong style=\"box-sizing: border-box; border-radius: 0px !important;\">Clustered up</strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">To enable robots to predict human movements, researchers typically borrow algorithms from music and speech processing. These algorithms are designed to align two complete time series, or sets of related data, such as an audio track of a musical performance and a scrolling video of that piece&#39;s musical notation.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">Researchers have used similar alignment algorithms to sync up real-time and previously recorded measurements of human motion, to predict where a person will be, say, five seconds from now. But unlike music or speech, human motion can be messy and highly variable. Even for repetitive movements, such as reaching across a table to screw in a bolt, one person may move slightly differently each time.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">Existing algorithms typically take in streaming motion data, in the form of dots representing the position of a person over time, and compare the trajectory of those dots to a library of common trajectories for the given scenario. An algorithm maps a trajectory in terms of the relative distance between dots.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">But Lasota says algorithms that predict trajectories based on distance alone can get easily confused in certain common situations, such as temporary stops, in which a person pauses before continuing on their path. While paused, dots representing the person&#39;s position can bunch up in the same spot.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">&quot;When you look at the data, you have a whole bunch of points clustered together when a person is stopped,&quot; Lasota says. &quot;If you&#39;re only looking at the distance between points as your alignment metric, that can be confusing, because they&#39;re all close together, and you don&#39;t have a good idea of which point you have to align to.&quot;</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">The same goes with overlapping trajectories -- instances when a person moves back and forth along a similar path. Lasota says that while a person&#39;s current position may line up with a dot on a reference trajectory, existing algorithms can&#39;t differentiate between whether that position is part of a trajectory heading away, or coming back along the same path.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">&quot;You may have points close together in terms of distance, but in terms of time, a person&#39;s position may actually be far from a reference point,&quot; Lasota says.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\"><strong style=\"box-sizing: border-box; border-radius: 0px !important;\">It&#39;s all in the timing</strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">As a solution, Lasota and Shah devised a &quot;partial trajectory&quot; algorithm that aligns segments of a person&#39;s trajectory in real-time with a library of previously collected reference trajectories. Importantly, the new algorithm aligns trajectories in both distance and timing, and in so doing, is able to accurately anticipate stops and overlaps in a person&#39;s path.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">&quot;Say you&#39;ve executed this much of a motion,&quot; Lasota explains. &quot;Old techniques will say, &#39;this is the closest point on this representative trajectory for that motion.&#39; But since you only completed this much of it in a short amount of time, the timing part of the algorithm will say, &#39;based on the timing, it&#39;s unlikely that you&#39;re already on your way back, because you just started your motion.&#39;&quot;</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">The team tested the algorithm on two human motion datasets: one in which a person intermittently crossed a robot&#39;s path in a factory setting (these data were obtained from the team&#39;s experiments with BMW), and another in which the group previously recorded hand movements of participants reaching across a table to install a bolt that a robot would then secure by brushing sealant on the bolt.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">For both datasets, the team&#39;s algorithm was able to make better estimates of a person&#39;s progress through a trajectory, compared with two commonly used partial trajectory alignment algorithms. Furthermore, the team found that when they integrated the alignment algorithm with their motion predictors, the robot could more accurately anticipate the timing of a person&#39;s motion. In the factory floor scenario, for example, they found the robot was less prone to freezing in place, and instead smoothly resumed its task shortly after a person crossed its path.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">While the algorithm was evaluated in the context of motion prediction, it can also be used as a preprocessing step for other techniques in the field of human-robot interaction, such as action recognition and gesture detection. Shah says the algorithm will be a key tool in enabling robots to recognize and respond to patterns of human movements and behaviors. Ultimately, this can help humans and robots work together in structured environments, such as factory settings and even, in some cases, the home.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">&quot;This technique could apply to any environment where humans exhibit typical patterns of behavior,&quot; Shah says. &quot;The key is that the [robotic] system can observe patterns that occur over and over, so that it can learn something about human behavior. This is all in the vein of work of the robot better understand aspects of human motion, to be able to collaborate with us better.&quot;</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">This research was funded, in part, by a NASA Space Technology Research Fellowship and the National Science Foundation.</p><hr/><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\"><strong style=\"box-sizing: border-box; border-radius: 0px !important;\">Story Source:</strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\"><a href=\"http://news.mit.edu/2019/robots-predict-human-movement-0611\" target=\"_blank\" style=\"box-sizing: border-box; background-color: transparent; color: rgb(76, 122, 159); text-decoration-line: none; border-radius: 0px !important;\">Materials</a>&nbsp;provided by&nbsp;<a href=\"http://web.mit.edu/\" target=\"_blank\" style=\"box-sizing: border-box; background-color: transparent; color: rgb(76, 122, 159); text-decoration-line: none; border-radius: 0px !important;\"><strong style=\"box-sizing: border-box; border-radius: 0px !important;\">Massachusetts Institute of Technology</strong></a>. Original written by Jennifer Chu.&nbsp;<em style=\"box-sizing: border-box; border-radius: 0px !important;\">Note: Content may be edited for style and length.</em></p><p><br/></p>', '2019-07-10 21:38:29', 'Robotics', '0', 'Uploads/2019-07-10/5d25ea54d6967.jpg');
INSERT INTO `article` VALUES ('39', 'Somebody\'s watching you: The surveillance of self-driving cars', '<p class=\"lead\" style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 20px; font-size: 18px; line-height: 1.4; color: rgb(51, 51, 51); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; white-space: normal; background-color: rgb(255, 255, 255); border-radius: 0px !important;\"><strong>Picture the future, where driving is a thing of the past. You can hop in your car or one from a ride-share, buckle up and tell the car where you want to go. During your ride, you can check your email and look up a few things online through your dashboard. Meanwhile, your whereabouts and other details are being tracked remotely by companies. As self-driving cars develop further, autonomous vehicles will play a much larger role in the digital economy as car companies and others harness personalized customer information through geospatial and navigation technologies, combining it with existing financial consumer profiles, according to a study in&nbsp;<em style=\"box-sizing: border-box; border-radius: 0px !important;\">Surveillance and Society</em>.</strong></p><p class=\"lead\" style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 20px; font-size: 18px; line-height: 1.4; color: rgb(51, 51, 51); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; white-space: normal; background-color: rgb(255, 255, 255); border-radius: 0px !important; text-align: center;\"><strong><img src=\"/ueditor/php/upload/image/20190710/1562765260134095.jpg\" title=\"1562765260134095.jpg\" alt=\"190607140431_1_540x360.jpg\" width=\"377\" height=\"247\"/></strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">&quot;Self-driving cars will represent a new mode for surveillance. Through a self-driving car&#39;s global positioning, system, navigational tools, and other data collection mechanisms, companies will be able to gain access to highly contextual data about passengers&#39; habits, routines, movements, and preferences,&quot; explained Luis F. Alvarez León, an assistant professor of geography at Dartmouth. &quot;This trove of personal, locational, and financial data can be leveraged and monetized by companies, by providing a data-stream for companies to target customers through personalized advertising and marketing,&quot; he added.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">Today&#39;s cars are already spatial multimedia environments that are highly computerized but self-driving cars will take this to a whole new level. They will also enable passengers to spend more time engaging with media in a vehicle. As the study point outs, this new economy may challenge notions of traditional car ownership, transforming &quot;the car into a bundle of services rather than just a product.&quot; Automobile manufacturers may essentially become digital platforms for media companies, search engines, retailers, vendors, and other companies, aiming to offer services to passengers through a car&#39;s infotainment system.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">The growth of self-driving cars will require more extensive communication networks, which will benefit ride-sharing companies, automobile manufacturers and other companies entering this new information-centric space. &quot;Through autonomous cars, the automotive and technology industries are likely to become more integrated with synergies across geospatial, navigation, artificial intelligence, ride-hailing, automotive and other industries and technologies,&quot; said Alvarez Léon.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">As self-driving car technologies develop, privacy and security concerns loom as to how companies will use personal data, an area for which the limits and specific governance mechanisms have yet to be defined by federal regulations.</p><hr/><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\"><strong style=\"box-sizing: border-box; border-radius: 0px !important;\">Story Source:</strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\"><a href=\"https://www.dartmouth.edu/press-releases/surveillance_of_self_driving_cars_060719.html\" target=\"_blank\" style=\"box-sizing: border-box; background-color: transparent; color: rgb(76, 122, 159); text-decoration-line: none; border-radius: 0px !important;\">Materials</a>&nbsp;provided by&nbsp;<a href=\"http://www.dartmouth.edu/\" target=\"_blank\" style=\"box-sizing: border-box; background-color: transparent; color: rgb(76, 122, 159); text-decoration-line: none; border-radius: 0px !important;\"><strong style=\"box-sizing: border-box; border-radius: 0px !important;\">Dartmouth College</strong></a>.&nbsp;<em style=\"box-sizing: border-box; border-radius: 0px !important;\">Note: Content may be edited for style and length.</em></p><p><br/></p>', '2019-07-10 21:38:49', 'Robotics', '0', 'Uploads/2019-07-10/5d25ea6890b58.jpg');
INSERT INTO `article` VALUES ('40', 'Autonomous boats can target and latch onto each other', '<p class=\"lead\" style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 20px; font-size: 18px; line-height: 1.4; color: rgb(51, 51, 51); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; white-space: normal; background-color: rgb(255, 255, 255); border-radius: 0px !important;\"><strong>The city of Amsterdam envisions a future where fleets of autonomous boats cruise its many canals to transport goods and people, collect trash, or self-assemble into floating stages and bridges. To further that vision, MIT researchers have given new capabilities to their fleet of robotic boats -- which are being developed as part of an ongoing project -- that let them target and clasp onto each other, and keep trying if they fail.</strong></p><p class=\"lead\" style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 20px; font-size: 18px; line-height: 1.4; color: rgb(51, 51, 51); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; white-space: normal; background-color: rgb(255, 255, 255); border-radius: 0px !important; text-align: center;\"><strong><img src=\"/ueditor/php/upload/image/20190710/1562765374469476.jpg\" title=\"1562765374469476.jpg\" alt=\"190605134857_1_540x360.jpg\" width=\"427\" height=\"246\"/></strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">About a quarter of Amsterdam&#39;s surface area is water, with 165 canals winding alongside busy city streets. Several years ago, MIT and the Amsterdam Institute for Advanced Metropolitan Solutions (AMS Institute) teamed up on the &quot;Roboat&quot; project. The idea is to build a fleet of autonomous robotic boats -- rectangular hulls equipped with sensors, thrusters, microcontrollers, GPS modules, cameras, and other hardware -- that provides intelligent mobility on water to relieve congestion in the city&#39;s busy streets.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">One of project&#39;s objectives is to create roboat units that provide on-demand transportation on waterways. Another objective is using the roboat units to automatically form &quot;pop-up&quot; structures, such as foot bridges, performance stages, or even food markets. The structures could then automatically disassemble at set times and reform into target structures for different activities. Additionally, the roboat units could be used as agile sensors to gather data on the city&#39;s infrastructure, and air and water quality, among other things.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">In 2016, MIT researchers tested a roboat prototype that cruised around Amsterdam&#39;s canals, moving forward, backward, and laterally along a preprogrammed path. Last year, researchers designed low-cost, 3-D-printed, one-quarter scale versions of the boats, which were more efficient and agile, and came equipped with advanced trajectory-tracking algorithms.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">In a paper presented at the International Conference on Robotics and Automation, the researchers describe roboat units that can now identify and connect to docking stations. Control algorithms guide the roboats to the target, where they automatically connect to a customized latching mechanism with millimeter precision. Moreover, the roboat notices if it has missed the connection, backs up, and tries again.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">The researchers tested the latching technique in a swimming pool at MIT and in the Charles River, where waters are rougher. In both instances, the roboat units were usually able to successfully connect in about 10 seconds, starting from around 1 meter away, or they succeeded after a few failed attempts. In Amsterdam, the system could be especially useful for overnight garbage collection. Roboat units could sail around a canal, locate and latch onto platforms holding trash containers, and haul them back to collection facilities.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">&quot;In Amsterdam, canals were once used for transportation and other things the roads are now used for. Roads near canals are now very congested -- and have noise and pollution -- so the city wants to add more functionality back to the canals,&quot; says first author Luis Mateos, a graduate student in the Department of Urban Studies and Planning (DUSP) and a researcher in the MIT Senseable City Lab. &quot;Self-driving technologies can save time, costs and energy, and improve the city moving forward.&quot;</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">&quot;The aim is to use roboat units to bring new capabilities to life on the water,&quot; adds co-author Daniela Rus, director of the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science. &quot;The new latching mechanism is very important for creating pop-up structures. Roboat does not need latching for autonomous transportation on water, but you need the latching to create any structure, whether it&#39;s mobile or fixed.&quot;</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">Joining Mateos on the paper are: Wei Wang, a joint postdoc in CSAIL and the Senseable City Lab; Banti Gheneti, a graduate student in the Department of Electrical Engineering and Computer Science; Fabio Duarte, a DUSP and Senseable City Lab research scientist; and Carlo Ratti, director of the Senseable City Lab and a principal investigator and professor of the practice in DUSP.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\"><strong style=\"box-sizing: border-box; border-radius: 0px !important;\">Making the connection</strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">Each roboat is equipped with latching mechanisms, including ball and socket components, on its front, back, and sides. The ball component resembles a badminton shuttlecock -- a cone-shaped, rubber body with a metal ball at the end. The socket component is a wide funnel that guides the ball component into a receptor. Inside the funnel, a laser beam acts like a security system that detects when the ball crosses into the receptor. That activates a mechanism with three arms that closes around and captures the ball, while also sending a feedback signal to both roboats that the connection is complete.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">On the software side, the roboats run on custom computer vision and control techniques. Each roboat has a LIDAR system and camera, so they can autonomously move from point to point around the canals. Each docking station -- typically an unmoving roboat -- has a sheet of paper imprinted with an augmented reality tag, called an AprilTag, which resembles a simplified QR code. Commonly used for robotic applications, AprilTags enable robots to detect and compute their precise 3-D position and orientation relative to the tag.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">Both the AprilTags and cameras are located in the same locations in center of the roboats. When a traveling roboat is roughly one or two meters away from the stationary AprilTag, the roboat calculates its position and orientation to the tag. Typically, this would generate a 3-D map for boat motion, including roll, pitch, and yaw (left and right). But an algorithm strips away everything except yaw. This produces an easy-to-compute 2-D plane that measures the roboat camera&#39;s distance away and distance left and right of the tag. Using that information, the roboat steers itself toward the tag. By keeping the camera and tag perfectly aligned, the roboat is able to precisely connect.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">The funnel compensates for any misalignment in the roboat&#39;s pitch (rocking up and down) and heave (vertical up and down), as canal waves are relatively small. If, however, the roboat goes beyond its calculated distance, and doesn&#39;t receive a feedback signal from the laser beam, it knows it has missed. &quot;In challenging waters, sometimes roboat units at the current one-quarter scale, are not strong enough to overcome wind gusts or heavy water currents,&quot; Mateos says. &quot;A logic component on the roboat says, &#39;You missed, so back up, recalculate your position, and try again.&#39;&quot;</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\"><strong style=\"box-sizing: border-box; border-radius: 0px !important;\">Future iterations</strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">The researchers are now designing roboat units roughly four times the size of the current iterations, so they&#39;ll be more stable on water. Mateos is also working on an update to the funnel that includes tentacle-like rubber grippers that tighten around the pin -- like a squid grasping its prey. That could help give the roboat units more control when, say, they&#39;re towing platforms or other roboats through narrow canals.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">In the works is also a system that displays the AprilTags on an LCD monitor that changes codes to signal multiple roboat units to assemble in a given order. At first, all roboat units will be given a code to stay exactly a meter apart. Then, the code changes to direct the first roboat to latch. After, the screen switches codes to order the next roboat to latch, and so on. &quot;It&#39;s like the telephone game. The changing code passes a message to one roboat at a time, and that message tells them what to do,&quot; Mateos says.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">The research was funded by the AMS Institute and the City of Amsterdam.</p><hr/><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\"><strong style=\"box-sizing: border-box; border-radius: 0px !important;\">Story Source:</strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\"><a href=\"http://news.mit.edu/2019/autonomous-robot-boats-latch-0605\" target=\"_blank\" style=\"box-sizing: border-box; background-color: transparent; color: rgb(76, 122, 159); text-decoration-line: none; border-radius: 0px !important;\">Materials</a>&nbsp;provided by&nbsp;<a href=\"http://web.mit.edu/\" target=\"_blank\" style=\"box-sizing: border-box; background-color: transparent; color: rgb(76, 122, 159); text-decoration-line: none; border-radius: 0px !important;\"><strong style=\"box-sizing: border-box; border-radius: 0px !important;\">Massachusetts Institute of Technology</strong></a>. Original written by Rob Matheson.&nbsp;<em style=\"box-sizing: border-box; border-radius: 0px !important;\">Note: Content may be edited for style and length.</em></p><p><em style=\"box-sizing: border-box; border-radius: 0px !important;\"><br/></em></p><p><br/></p>', '2019-07-10 21:39:10', 'Robotics', '0', 'Uploads/2019-07-10/5d25ea7d88cb1.jpg');
INSERT INTO `article` VALUES ('41', '\'Neural Lander\' uses AI to land drones smoothly', '<p class=\"lead\" style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 20px; font-size: 18px; line-height: 1.4; color: rgb(51, 51, 51); font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; white-space: normal; background-color: rgb(255, 255, 255); border-radius: 0px !important;\"><strong>Landing multi-rotor drones smoothly is difficult. Complex turbulence is created by the airflow from each rotor bouncing off the ground as the ground grows ever closer during a descent. This turbulence is not well understood nor is it easy to compensate for, particularly for autonomous drones. That is why takeoff and landing are often the two trickiest parts of a drone flight. Drones typically wobble and inch slowly toward a landing until power is finally cut, and they drop the remaining distance to the ground.</strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">At Caltech&#39;s Center for Autonomous Systems and Technologies (CAST), artificial intelligence experts have teamed up with control experts to develop a system that uses a deep neural network to help autonomous drones &quot;learn&quot; how to land more safely and quickly, while gobbling up less power. The system they have created, dubbed the &quot;Neural Lander,&quot; is a learning-based controller that tracks the position and speed of the drone, and modifies its landing trajectory and rotor speed accordingly to achieve the smoothest possible landing.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">&quot;This project has the potential to help drones fly more smoothly and safely, especially in the presence of unpredictable wind gusts, and eat up less battery power as drones can land more quickly,&quot; says Soon-Jo Chung, Bren Professor of Aerospace in the Division of Engineering and Applied Science (EAS) and research scientist at JPL, which Caltech manages for NASA. The project is a collaboration between Chung and Caltech artificial intelligence (AI) experts Anima Anandkumar, Bren Professor of Computing and Mathematical Sciences, and Yisong Yue, assistant professor of computing and mathematical sciences.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">A paper describing the Neural Lander will be presented at the Institute of Electrical and Electronics Engineers (IEEE) International Conference on Robotics and Automation on May 22. Co-lead authors of the paper are Caltech graduate students Guanya Shi, whose PhD research is jointly supervised by Chung and Yue, as well as Xichen Shi and Michael O&#39;Connell, who are the PhD students in Chung&#39;s Aerospace Robotics and Control Group.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">Deep neural networks (DNNs) are AI systems that are inspired by biological systems like the brain. The &quot;deep&quot; part of the name refers to the fact that data inputs are churned through multiple layers, each of which processes incoming information in a different way to tease out increasingly complex details. DNNs are capable of automatic learning, which makes them ideally suited for repetitive tasks.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">To make sure that the drone flies smoothly under the guidance of the DNN, the team employed a technique known as spectral normalization, which smooths out the neural net&#39;s outputs so that it doesn&#39;t make wildly varying predictions as inputs/conditions shift. Improvements in landing were measured by examining deviation from an idealized trajectory in 3D space. Three types of tests were conducted: a straight vertical landing; a descending arc landing; and flight in which the drone skims across a broken surface -- such as over the edge of a table -- where the effect of turbulence from the ground would vary sharply.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">The new system decreases vertical error by 100 percent, allowing for controlled landings, and reduces lateral drift by up to 90 percent. In their experiments, the new system achieves actual landing rather than getting stuck about 10 to 15 centimeters above the ground, as unmodified conventional flight controllers often do. Further, during the skimming test, the Neural Lander produced a much a smoother transition as the drone transitioned from skimming across the table to flying in the free space beyond the edge.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">&quot;With less error, the Neural Lander is capable of a speedier, smoother landing and of gliding smoothly over the ground surface,&quot; Yue says. The new system was tested at CAST&#39;s three-story-tall aerodrome, which can simulate a nearly limitless variety of outdoor wind conditions. Opened in 2018, CAST is a 10,000-square-foot facility where researchers from EAS, JPL, and Caltech&#39;s Division of Geological and Planetary Sciences are uniting to create the next generation of autonomous systems, while advancing the fields of drone research, autonomous exploration, and bioinspired systems.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">&quot;This interdisciplinary effort brings experts from machine learning and control systems. We have barely started to explore the rich connections between the two areas,&quot; Anandkumar says.</p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\">Besides its obvious commercial applications -- Chung and his colleagues have filed a patent on the new system -- the new system could prove crucial to projects currently under development at CAST, including an autonomous medical transport that could land in difficult-to-reach locations (such as a gridlocked traffic). &quot;The importance of being able to land swiftly and smoothly when transporting an injured individual cannot be overstated,&quot; says Morteza Gharib, Hans W. Liepmann Professor of Aeronautics and Bioinspired Engineering; director of CAST; and one of the lead researchers of the air ambulance project.</p><hr/><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\"><strong style=\"box-sizing: border-box; border-radius: 0px !important;\">Story Source:</strong></p><p style=\"box-sizing: border-box; margin-top: 0px; margin-bottom: 10px; border-radius: 0px !important;\"><a href=\"https://www.caltech.edu/about/news/neural-lander-uses-ai-land-drones-smoothly\" target=\"_blank\" style=\"box-sizing: border-box; background-color: transparent; color: rgb(76, 122, 159); text-decoration-line: none; border-radius: 0px !important;\">Materials</a>&nbsp;provided by&nbsp;<a href=\"http://www.caltech.edu/\" target=\"_blank\" style=\"box-sizing: border-box; background-color: transparent; color: rgb(76, 122, 159); text-decoration-line: none; border-radius: 0px !important;\"><strong style=\"box-sizing: border-box; border-radius: 0px !important;\">California Institute of Technology</strong></a>. Original written by Robert Perkins.&nbsp;<em style=\"box-sizing: border-box; border-radius: 0px !important;\">Note: Content may be edited for style and length.</em></p><p><br/></p>', '2019-07-10 21:44:56', 'Robotics', '0', 'Uploads/2019-07-10/5d25ebd7c2702.jpg');
INSERT INTO `article` VALUES ('42', 'A new way to use the AI behind deepfakes could improve cancer diagnosis', '<p class=\"jsx-1279085685\" style=\"box-sizing: inherit; margin-top: 0px; margin-bottom: 14px; color: rgb(48, 48, 48); font-family: NeueHaas, sans-serif; font-size: 18px; white-space: normal; background-color: rgb(255, 255, 255); text-align: center;\"><span style=\"text-decoration: none;\"><strong><img src=\"/ueditor/php/upload/image/20190710/1562773153594785.jpg\" title=\"1562773153594785.jpg\" alt=\"ap16349375278538.jpg\" width=\"521\" height=\"276\"/></strong></span></p><p class=\"jsx-1279085685\" style=\"box-sizing: inherit; margin-top: 0px; margin-bottom: 14px; color: rgb(48, 48, 48); font-family: NeueHaas, sans-serif; font-size: 18px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"text-decoration: none;\"><strong><br/></strong></span></p><p class=\"jsx-1279085685\" style=\"box-sizing: inherit; margin-top: 0px; margin-bottom: 14px; color: rgb(48, 48, 48); font-family: NeueHaas, sans-serif; font-size: 18px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"text-decoration: none;\"><strong><span style=\"text-decoration: none; box-sizing: inherit;\"><a class=\"\" href=\"https://www.technologyreview.com/s/612501/inside-the-world-of-ai-that-forges-beautiful-art-and-terrifying-deepfakes/\" style=\"box-sizing: inherit;\">Generative adversarial networks</a></span></strong></span><span style=\"text-decoration: none;\"><strong>,</strong><strong> the algorithms responsible for deepfakes, have developed a bit of a bad rap of late. But their ability to synthesize highly realistic images could also have important benefits for medical diagnosis.</strong></span></p><p class=\"jsx-1279085685\" style=\"box-sizing: inherit; margin-top: 0px; margin-bottom: 14px; color: rgb(48, 48, 48); font-family: NeueHaas, sans-serif; font-size: 18px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"text-decoration: none;\"><strong>Deep-learning algorithms are excellent at pattern-matching in images; they can be trained to detect different types of cancer in a CT scan, differentiate diseases in MRIs, and identify abnormalities in an x-ray. But because of privacy concerns, researchers often don’t have enough training data. This is where GANs come in: they can synthesize more medical images that are indistinguishable from the real ones, effectively multiplying a data set to the necessary quantity.</strong></span></p><p class=\"jsx-1279085685\" style=\"box-sizing: inherit; margin-top: 0px; margin-bottom: 14px; color: rgb(48, 48, 48); font-family: NeueHaas, sans-serif; font-size: 18px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"text-decoration: none;\"><strong>There is another challenge, though. Deep-learning algorithms need to train on high-resolution images to produce the best predictions, yet synthesizing such high-res images, especially in 3D, takes a lot of computational power. That means it requires special and expensive hardware, making its large-scale use impractical in hospitals.</strong></span></p><p class=\"jsx-1279085685\" style=\"box-sizing: inherit; margin-top: 0px; margin-bottom: 14px; color: rgb(48, 48, 48); font-family: NeueHaas, sans-serif; font-size: 18px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"text-decoration: none;\"><strong>So researchers from the Institute of Medical Informatics at the University of Lübeck&nbsp;<span class=\"jsx-2473920458 storyLink\" style=\"text-decoration: none; box-sizing: inherit;\"><a class=\"\" href=\"https://arxiv.org/abs/1907.01376\" style=\"box-sizing: inherit;\">proposed a new approach</a></span></strong><strong>&nbsp;to make the process much less intensive. They broke it up into stages: the GAN first generates the whole image in low-res, then generates the details at the right resolution one small section at a time. Through experiments, the researchers showed not only that their method generated realistic high-res 2D and 3D images with low computational resources, but that the expenditure also stayed constant regardless of image size.</strong></span></p><p><br/></p>', '2019-07-10 23:39:41', 'Computer Vision', '0', 'Uploads/2019-07-10/5d2606bc94863.jpg');
INSERT INTO `article` VALUES ('43', 'Google Photos to be updated with manual face tagging and more', '<p style=\"color: rgb(34, 34, 34); font-family: &quot;Open Sans&quot;, Helvetica, Arial, sans-serif; font-size: 14px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">Google Photos will soon be updated with several new features we&#39;ve heard about through a slightly unconventional communications channel. Google Photos product lead David Lieb made the most of some unexpected downtime this week by asking Twitter users what kind of features they would like to see next in Google Photos.</span></p><p style=\"color: rgb(34, 34, 34); font-family: &quot;Open Sans&quot;, Helvetica, Arial, sans-serif; font-size: 14px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">In the end, the message exchange went on for hours and revealed some of the new features and functions Google Photo users should be looking forward to in the nearer future. One of them is a manual face tagging option that will let you tag faces that have not been automatically tagged by the face recognition algorithms.</span></p><p style=\"color: rgb(34, 34, 34); font-family: &quot;Open Sans&quot;, Helvetica, Arial, sans-serif; font-size: 14px; white-space: normal; background-color: rgb(255, 255, 255); text-align: center;\"><span style=\"font-size: 16px;\"><img src=\"/ueditor/php/upload/image/20190710/1562773381370374.jpeg\" title=\"1562773381370374.jpeg\" alt=\"GOTY18-GooglePhoto-03.jpeg\" width=\"450\" height=\"279\"/></span></p><p style=\"color: rgb(34, 34, 34); font-family: &quot;Open Sans&quot;, Helvetica, Arial, sans-serif; font-size: 14px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">In addition, a few functions that already exist in the web version of Google Photos will also be available in the mobile app. Soon mobile users will be able to search for recently uploaded photos. In the Android version, you&#39;ll also be able to edit timestamps and delete photos while in an album, both functions that are already available in the iOS version.</span></p><p style=\"color: rgb(34, 34, 34); font-family: &quot;Open Sans&quot;, Helvetica, Arial, sans-serif; font-size: 14px; white-space: normal; background-color: rgb(255, 255, 255);\"><span style=\"font-size: 16px;\">There&#39;s no exact schedule for the availability of the new features, nor do we know if any of the features suggested by users—for example a map feature, removing duplicates and showing photo descriptions in slide shows, will ever make it in the app—but it&#39;s good to see a company like Google communicate with users in such an open way.</span></p><p><br/></p>', '2019-07-10 23:43:16', 'Computer Vision', '0', 'Uploads/2019-07-10/5d260793048e4.jpeg');

-- ----------------------------
-- Table structure for comment
-- ----------------------------
DROP TABLE IF EXISTS `comment`;
CREATE TABLE `comment` (
  `cid` int(100) NOT NULL AUTO_INCREMENT,
  `aid` int(100) NOT NULL,
  `uid` int(100) NOT NULL,
  `ctext` longtext NOT NULL,
  `ctime` datetime NOT NULL,
  PRIMARY KEY (`cid`),
  KEY `c_fk_1` (`aid`),
  KEY `c_fk_2` (`uid`)
) ENGINE=MyISAM AUTO_INCREMENT=16 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of comment
-- ----------------------------

-- ----------------------------
-- Table structure for group
-- ----------------------------
DROP TABLE IF EXISTS `group`;
CREATE TABLE `group` (
  `gid` int(100) NOT NULL AUTO_INCREMENT,
  `gname` varchar(100) NOT NULL,
  `gimage` varchar(100) DEFAULT NULL,
  `ginfo` varchar(255) DEFAULT NULL,
  `uid` int(100) NOT NULL,
  PRIMARY KEY (`gid`),
  KEY `g_fk_1` (`uid`)
) ENGINE=MyISAM AUTO_INCREMENT=14 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of group
-- ----------------------------
INSERT INTO `group` VALUES ('9', 'Mechine Learing', 'Uploads/2019-07-11/5d275b6d17e51.jpg', 'Welcome people who are keen on ML', '17');
INSERT INTO `group` VALUES ('10', 'Code Python', 'Uploads/2019-07-11/5d275bd2cc4ab.jpeg', 'Learning group for python coding', '17');
INSERT INTO `group` VALUES ('11', 'OpenCV Application', 'Uploads/2019-07-11/5d275c984bf37.jpg', 'OpenCV for everyone!', '17');
INSERT INTO `group` VALUES ('12', 'Optimization Algorithm', 'Uploads/2019-07-12/5d275d2e6ee9f.jpg', 'Let figure out algorithm together !', '17');
INSERT INTO `group` VALUES ('13', 'Image Processing', 'Uploads/2019-07-12/5d27601f42bd7.jpg', 'If you are working on image processing, you should join us !', '17');

-- ----------------------------
-- Table structure for member
-- ----------------------------
DROP TABLE IF EXISTS `member`;
CREATE TABLE `member` (
  `mid` int(100) NOT NULL AUTO_INCREMENT,
  `gid` int(100) NOT NULL,
  `uid` int(100) NOT NULL,
  PRIMARY KEY (`mid`),
  KEY `m_fk_1` (`gid`),
  KEY `m_fk_2` (`uid`)
) ENGINE=MyISAM AUTO_INCREMENT=24 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of member
-- ----------------------------
INSERT INTO `member` VALUES ('16', '9', '17');
INSERT INTO `member` VALUES ('17', '10', '17');
INSERT INTO `member` VALUES ('18', '11', '17');
INSERT INTO `member` VALUES ('19', '12', '17');
INSERT INTO `member` VALUES ('20', '13', '17');

-- ----------------------------
-- Table structure for reply
-- ----------------------------
DROP TABLE IF EXISTS `reply`;
CREATE TABLE `reply` (
  `rid` int(100) NOT NULL AUTO_INCREMENT,
  `tid` int(100) NOT NULL,
  `uid` int(100) NOT NULL,
  `rtext` longtext NOT NULL,
  `rtime` datetime DEFAULT NULL,
  PRIMARY KEY (`rid`),
  KEY `r_fk_1` (`tid`),
  KEY `r_fk_2` (`uid`)
) ENGINE=MyISAM AUTO_INCREMENT=8 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of reply
-- ----------------------------

-- ----------------------------
-- Table structure for topic
-- ----------------------------
DROP TABLE IF EXISTS `topic`;
CREATE TABLE `topic` (
  `tid` int(100) NOT NULL AUTO_INCREMENT,
  `ttitle` varchar(255) NOT NULL,
  `uid` int(100) NOT NULL,
  `gid` int(100) NOT NULL,
  `ttext` longtext,
  `ttime` datetime DEFAULT NULL,
  `tlike` int(100) DEFAULT '0',
  PRIMARY KEY (`tid`),
  KEY `t_fk_1` (`uid`),
  KEY `t_fk_2` (`gid`)
) ENGINE=MyISAM AUTO_INCREMENT=10 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of topic
-- ----------------------------
INSERT INTO `topic` VALUES ('9', 'How to learn ML?', '17', '9', '<p>please give me some advice!</p>', '2019-08-11 22:06:17', '0');
INSERT INTO `topic` VALUES ('8', 'It\'s a great group!', '17', '9', '<p>nice to meet you guys!</p>', '2019-08-11 22:05:33', '0');

-- ----------------------------
-- Table structure for user
-- ----------------------------
DROP TABLE IF EXISTS `user`;
CREATE TABLE `user` (
  `uid` int(100) NOT NULL AUTO_INCREMENT,
  `username` varchar(100) NOT NULL,
  `pwd` varchar(255) NOT NULL,
  `name` varchar(100) NOT NULL,
  `tag` int(5) NOT NULL DEFAULT '1',
  `sex` varchar(5) DEFAULT NULL,
  `email` varchar(100) DEFAULT NULL,
  `image` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`uid`)
) ENGINE=MyISAM AUTO_INCREMENT=37 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of user
-- ----------------------------
INSERT INTO `user` VALUES ('17', 'Stark95', '$2y$10$Uz1XS5p3FUbox/5qjdL4LOdCbp0BSBLzAHp6SJQkhe6Z.NaypqtxW', 'Stark', '1', 'male', '834176915@qq.com', 'Uploads/2019-08-16/5d55d05dc41ac.jpg');
INSERT INTO `user` VALUES ('23', 'admin', '$2y$10$Yo3h5VnVS7mVINqbv3xfIuTVZ8PqT5EwH1BReyoj.KIlsXBJsP31S', 'admin', '0', null, null, null);
INSERT INTO `user` VALUES ('35', 'Tony', '$2y$10$Ev5OyPerXLtuyuntJzuNQeIc/GvgPwWE67s43AR8KbLy8EAxK0xgu', 'Tony', '1', 'male', '1315648916@google.com', 'Uploads/2019-08-16/5d56be62c3d0a.jpg');
INSERT INTO `user` VALUES ('36', 'xxz', '$2y$10$FEHanWWpODj24dJEEUXsVOvTbJ71NVWWFrSAKAt88Syp1XrnRjL7q', 'xxz', '1', 'male', '1111111@aaaaaaaa.com', 'Public/images/user.png');
